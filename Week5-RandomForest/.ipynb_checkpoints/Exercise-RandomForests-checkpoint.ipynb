{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries \n",
    "\n",
    "# Pandas and numpy for data wrangling\n",
    "import pandas as pd\n",
    "import numpy np\n",
    "# Seaborn / matplotlib for visualization \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Import the trees from sklearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Helper function to split our data\n",
    "from \n",
    "\n",
    "# Helper fuctions to evaluate our model.\n",
    "\n",
    "\n",
    "# Helper function for hyper-parameter turning.\n",
    "\n",
    "\n",
    "# Import our Decision Tree\n",
    "\n",
    "\n",
    "# Import our Random Forest \n",
    "\n",
    "\n",
    "# Library for visualizing our tree\n",
    "# If you get an error, run 'conda install python-graphviz' in your terminal (without the quotes).\n",
    "import graphviz \n",
    "\n",
    "# Use inline so our visualizations display in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Steps when building a Machine Learning Model. \n",
    "1. Inspect and explore data.\n",
    "2. Select and engineer features.\n",
    "3. Build and train model.\n",
    "4. Evaluate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1 Inspect and explore data.\n",
    "* Load titanic data\n",
    "* Visualize all the data using sns.pairplot\n",
    "* Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the titanic data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all the data using sns.pairplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2 Select and engineer features.\n",
    "1. Fill age null values with -999\n",
    "1. Convert to numerical values if need be by using `pd.get_dummies()`\n",
    "1. Create a list of the features you are going to use.  In this case use as many or as little as you would like.\n",
    "1. Define our `X` and `y`\n",
    "1. Split our data into trainig and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill age null values with -999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to numerical values if need be by using `pd.get_dummies()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a list of the features we are going to use.\n",
    "selected_features = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our `X` and `y`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into trainig and testing sets.\n",
    "\n",
    "print('Lenght of our Training data:', ???, '\\nLength of our Testing data:', ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3 Build and train model.\n",
    "1. For our first pass, initialize our model with `max_depth=2`.\n",
    "2. Fit our model with our training data. \n",
    "3. Make predictions of our testing data. \n",
    "4. Evaluate and print our model scores using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "    * To calculate auc score you have to get the predicted probabilites for the Survived class using `model.predict_proba(X_test)[:,1]`\n",
    "5. Visualize our Decision Tree using provided code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our first pass, initialize our model with `max_depth=2`.\n",
    "\n",
    "model = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model with our training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions of our testing data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate and print our model scores using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "accuracy = ???\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "precision = ???\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "\n",
    "recall = ???\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "\n",
    "f1 = ???\n",
    "print('F1 Score: %f' % f1)\n",
    "\n",
    "# Calculate predicted probabilities\n",
    "y_pred_proba = model.predict_proba(???)\n",
    "\n",
    "# Keep only the proba for True\n",
    "y_pred_proba = y_pred_proba[:,1]\n",
    "\n",
    "# Compute auc score\n",
    "auc = ???\n",
    "print('AUC Score: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL:  Visualize your decision tree. \n",
    "* If you get an error, you may need to install the graphviz library.\n",
    "* Run this command in your terminal to install the graphviz library. \n",
    "    * `conda install python-graphviz`\n",
    "* If that does not work, then try installing the library using pip. \n",
    "    * `pip install graphviz`\n",
    "\n",
    "* If neither of those work, you can just skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL \n",
    "# Visualize your decision tree. \n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                     feature_names=selected_features,\n",
    "                     class_names=['died','survived'],\n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking the right parameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning of your Decision Tree using GridSearch\n",
    "\n",
    "1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "1. Initalize your GridSearchCV with a DecisionTreeClassifier, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "1. Fit your GridSearchCV with your training data. \n",
    "1. Print the parameters of your best model. \n",
    "1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "1. Visualize your best tree.\n",
    "1. Which feature was your most important feature?\n",
    "\n",
    "```python\n",
    "tree.DecisionTreeClassifier(\n",
    "    *,\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort='deprecated',\n",
    "    ccp_alpha=0.0,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tips on how to customize / set the paramters in the decision tree.](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search.from sklearn.model_selection import GridSearchCV\n",
    "params = { \n",
    "    'PARAMETER_NAME': ['LIST', 'OF', 'VALUES'], ??? }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize your GridSearchCV with a DecisionTreeClassifier, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "grid_search_cv =  GridSearchCV( ??? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit your GridSearchCV with your training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the parameters of your best model. \n",
    "# Print the best parameters it found\n",
    "print( ??? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "\n",
    "# This command gives you the best tree\n",
    "model = ???\n",
    "\n",
    "# Now lets evaluate our model\n",
    "y_pred = ???\n",
    "\n",
    "accuracy = ???\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "precision = ???\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "\n",
    "recall = ???\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "\n",
    "f1 = ???\n",
    "print('F1 Score: %f' % f1)\n",
    "\n",
    "# Calculate predicted probabilities, keep only probability for when class = 1\n",
    "y_pred_proba = model.predict_proba(???)[:,1]\n",
    "auc = ???\n",
    "print('AUC Score: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL!\n",
    "\n",
    "# 1. Visualize your best tree\n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                     feature_names=selected_features,\n",
    "                     class_names=['died','survived'],\n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Which feature was your most important feature?\n",
    "# Now lets look at our feature importances\n",
    "feature_imp = pd.DataFrame.from_dict( {'feature_importance': model.feature_importances_,\n",
    "                                       'feature':selected_features }).sort_values('feature_importance', ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now onto Random Forests...\n",
    "Were going to do the same with, but this time with a random forest. Remeber... Repetition is the father of learning.\n",
    "\n",
    "1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "1. Initalize your GridSearchCV with a RandomForestClassifer, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "1. Fit your GridSearchCV with your training data. \n",
    "1. Print the parameters of your best model. \n",
    "1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "1. Which feature was your most important feature?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    *,\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "params = {'PARAMETER_NAME': ['LIST', 'OF', 'VALUES'], ???\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize your GridSearchCV with a RandomForestClassifer, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "\n",
    "grid_search_cv = GridSearchCV( ??? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit your GridSearchCV with your training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the parameters of your best model. \n",
    "# Print the best parameters it found\n",
    "print(???)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "\n",
    "# This command gives you tree that has the highest f1-score. \n",
    "model = grid_search_cv.best_estimator_\n",
    "\n",
    "\n",
    "# Now lets evaluate our model\n",
    "y_pred = ???\n",
    "\n",
    "accuracy = ???\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "precision = ???\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "\n",
    "recall = ???\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "\n",
    "f1 = ???\n",
    "print('F1 Score: %f' % f1)\n",
    "\n",
    "# Calculate predicted probabilities, keep only probability for when class = 1\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc = ???\n",
    "print('AUC Score: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Which feature was your most important feature?\n",
    "# Now lets look at our feature importances\n",
    "feature_imp = pd.Series(model.feature_importances_,index=selected_features).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a random forest using the ny-vs-sf-housing.csv data. \n",
    "* Your target variable, aka the column you are trying to predict, aka your `y` variable is `in_sf`. \n",
    "* Can you get an accuracy above 91%?\n",
    "* What was your most important feature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ny-vs-sf-houses.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD, TRAIN, AND EVAULATE A RANDOM FOREST MODEL BELOW. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
